{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "41ce4b3f-2dc2-4147-9820-1f6e270c1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchmetrics import Accuracy, MetricCollection\n",
    "import segmentation_models_pytorch as smp\n",
    "from pytorch_toolbelt.losses import JaccardLoss, BinaryFocalLoss\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from typing import Any, Tuple, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec180b5b-d3fd-458c-9b91-a8295a911aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b316bb1-2cbe-4213-a055-6a5c11f0b8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "094d5e9f-7922-4bdb-bfa9-d904f70ce36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f59cd82-47b8-4566-93fa-882f13589c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sofija/Ai4er/mres/tree-segmentation'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/Users/sofija/Ai4er/mres/tree-segmentation\" )\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5f0d1d15-3094-4854-94ae-a4905ee078d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import TreeSegments\n",
    "from src.evaluation import BinaryIoU\n",
    "from src.datamodules import TreeDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c98ddd0-d3bb-4e58-b5f6-be6c3ed03880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c592048-c133-4bcf-9688-d7031c562eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.Unet(\n",
    "#     encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,                      # model output channels (number of classes in your dataset)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "305569c2-dcad-456a-b67e-3e1353b3e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TreeDataModule(\n",
    "    data_dir=\"data/test_depth/\",\n",
    "    target_dir=\"data/test_segment/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "25ea0dc2-6454-4d15-871e-ebdc73b587c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(stage=\"fit\")\n",
    "train_dl = dm.train_dataloader()\n",
    "val_dl = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1c3fe48c-ae00-47cb-ae55-8662c28e396b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sample in train_dl:\n",
    "    image = sample[0]\n",
    "    mask = sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cb61ad97-0eed-4d23-b4a7-618eaa1318c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright The PyTorch Lightning team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and/\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"Architecture based on U-Net: Convolutional Networks for Biomedical Image Segmentation.\n",
    "    Link - https://arxiv.org/abs/1505.04597\n",
    "    >>> UNet(num_classes=2, num_layers=3)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    UNet(\n",
    "      (layers): ModuleList(\n",
    "        (0): DoubleConv(...)\n",
    "        (1): Down(...)\n",
    "        (2): Down(...)\n",
    "        (3): Up(...)\n",
    "        (4): Up(...)\n",
    "        (5): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 1,\n",
    "        num_layers: int = 5,\n",
    "        features_start: int = 64,\n",
    "        bilinear: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: Number of output classes required (default 1 for binary segmentation)\n",
    "            num_layers: Number of layers in each side of U-net\n",
    "            features_start: Number of features in first layer\n",
    "            bilinear: Whether to use bilinear interpolation or transposed convolutions for upsampling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = [DoubleConv(3, features_start)]\n",
    "\n",
    "        feats = features_start\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Down(feats, feats * 2))\n",
    "            feats *= 2\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Up(feats, feats // 2, bilinear))\n",
    "            feats //= 2\n",
    "\n",
    "        layers.append(nn.Conv2d(feats, num_classes, kernel_size=1))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xi = [self.layers[0](x)]\n",
    "        # Down path\n",
    "        for layer in self.layers[1 : self.num_layers]:\n",
    "            xi.append(layer(xi[-1]))\n",
    "        # Up path\n",
    "        for i, layer in enumerate(self.layers[self.num_layers : -1]):\n",
    "            xi[-1] = layer(xi[-1], xi[-2 - i])\n",
    "        return self.layers[-1](xi[-1])\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Double Convolution and BN and ReLU (3x3 conv -> BN -> ReLU) ** 2.\n",
    "    >>> DoubleConv(4, 4)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    DoubleConv(\n",
    "      (net): Sequential(...)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Combination of MaxPool2d and DoubleConv in series.\n",
    "    >>> Down(4, 8)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    Down(\n",
    "      (net): Sequential(\n",
    "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        (1): DoubleConv(\n",
    "          (net): Sequential(...)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), DoubleConv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsampling (by either bilinear interpolation or transpose convolutions) followed by concatenation of feature\n",
    "    map from contracting path, followed by double 3x3 convolution.\n",
    "    >>> Up(8, 4)  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    Up(\n",
    "      (upsample): ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2))\n",
    "      (conv): DoubleConv(\n",
    "        (net): Sequential(...)\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int, bilinear: bool = False):\n",
    "        super().__init__()\n",
    "        self.upsample = None\n",
    "        if bilinear:\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
    "                nn.Conv2d(in_ch, in_ch // 2, kernel_size=1),\n",
    "            )\n",
    "        else:\n",
    "            self.upsample = nn.ConvTranspose2d(\n",
    "                in_ch, in_ch // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "\n",
    "        # Pad x1 to the size of x2\n",
    "        diff_h = x2.shape[2] - x1.shape[2]\n",
    "        diff_w = x2.shape[3] - x1.shape[3]\n",
    "\n",
    "        x1 = F.pad(\n",
    "            x1, [diff_w // 2, diff_w - diff_w // 2, diff_h // 2, diff_h - diff_h // 2]\n",
    "        )\n",
    "\n",
    "        # Concatenate along the channels axis\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1cf7bbe9-c7d7-452a-8959-f47eb563202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegModel(pl.LightningModule):\n",
    "    \"\"\"Semantic Segmentation Module.\n",
    "    This is a basic semantic segmentation module implemented with Lightning.\n",
    "    It uses CrossEntropyLoss as the default loss function. May be replaced with\n",
    "    other loss functions as required.\n",
    "\n",
    "    It uses the FCN ResNet50 model as an example.\n",
    "    Adam optimizer is used along with Cosine Annealing learning rate scheduler.\n",
    "\n",
    "    SegModel(\n",
    "      (net): UNet(\n",
    "        (layers): ModuleList(\n",
    "          (0): DoubleConv(...)\n",
    "          (1): Down(...)\n",
    "          (2): Down(...)\n",
    "          (3): Up(...)\n",
    "          (4): Up(...)\n",
    "          (5): Conv2d(64, 19, kernel_size=(1, 1), stride=(1, 1))\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 1,\n",
    "        batch_size: int = 4,\n",
    "        lr: float = 1e-3,\n",
    "        num_layers: int = 3,\n",
    "        features_start: int = 64,\n",
    "        bilinear: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.num_layers = num_layers\n",
    "        self.features_start = features_start\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.net = UNet(\n",
    "            num_classes=self.num_classes,\n",
    "            num_layers=self.num_layers,\n",
    "            features_start=self.features_start,\n",
    "            bilinear=self.bilinear,\n",
    "        )\n",
    "\n",
    "        self.train_metrics = MetricCollection(\n",
    "            [\n",
    "                Accuracy(\n",
    "                    num_classes=self.num_classes,\n",
    "                ),\n",
    "                # BinaryIoU(),\n",
    "            ],\n",
    "            prefix=\"train_\",\n",
    "        )\n",
    "        self.val_metrics = self.train_metrics.clone(prefix=\"val_\")\n",
    "        self.test_metrics = self.train_metrics.clone(prefix=\"test_\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        img, mask = batch\n",
    "        mask = mask.float()\n",
    "        out = self(img)\n",
    "        loss = nn.BCEWithLogitsLoss()(out, mask)\n",
    "        log_dict = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": log_dict, \"progress_bar\": log_dict}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, mask = batch\n",
    "        mask = mask.float()\n",
    "        out = self(img)\n",
    "        loss_val = nn.BCEWithLogitsLoss()(out, mask)\n",
    "        return {\"val_loss\": loss_val}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss_val = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        log_dict = {\"val_loss\": loss_val}\n",
    "        return {\n",
    "            \"log\": log_dict,\n",
    "            \"val_loss\": log_dict[\"val_loss\"],\n",
    "            \"progress_bar\": log_dict,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        # sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "        return [opt]\n",
    "        # return [opt], [sch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b29b6010-e6cb-4d37-8c07-3816034c7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5692594c-7064-4281-95e2-8329e3bd838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    # logger=wandb_logger,\n",
    "    # callbacks=callbacks,\n",
    "    # fast_dev_run=True,\n",
    "    max_epochs=5, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc887820-f049-4bd8-ab54-8ed473f0f797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | net  | UNet | 1.9 M \n",
      "------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.462     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  4.99it/s, loss=0.792, v_num=5]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                      | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.44it/s, loss=0.792, v_num=5]\u001b[A\n",
      "Epoch 1:  50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  5.35it/s, loss=0.716, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                      | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.96it/s, loss=0.716, v_num=5]\u001b[A\n",
      "Epoch 2:  50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  5.42it/s, loss=0.657, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                      | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  8.01it/s, loss=0.657, v_num=5]\u001b[A\n",
      "Epoch 3:  50%|█████████████████████████████████████                                     | 1/2 [00:00<00:00,  5.37it/s, loss=0.61, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                      | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.93it/s, loss=0.61, v_num=5]\u001b[A\n",
      "Epoch 4:  50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  5.38it/s, loss=0.571, v_num=5]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                      | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.92it/s, loss=0.571, v_num=5]\u001b[A\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.21it/s, loss=0.571, v_num=5]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c8b53ac1-a01c-4e4c-aea1-07fd8de665b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8b897-3832-4cb6-9b6b-909cc1b82793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
